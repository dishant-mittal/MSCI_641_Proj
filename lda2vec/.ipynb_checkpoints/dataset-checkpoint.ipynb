{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * \n",
    " * author Dishant Mittal\n",
    " * created on July 10, 2018\n",
    " * project MSCI 641\n",
    " \n",
    " * LDA2Vec is a model that learns dense word vectors jointly with Dirichlet-distributed latent \n",
    " * document-level mixtures of topic vectors. In contrast to continuous dense document \n",
    " * representations, this formulation produces sparse, interpretable document mixtures \n",
    " * through a non-negative simplex constraint.\n",
    " *\n",
    " \n",
    " * In this script, I load the data, clean it, create word2vec, bow, doc_ids, pruned words and flattened vecs.\n",
    " * Finally, I save all the objects.\n",
    " '''\n",
    "\n",
    "import logging\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import csv\n",
    "import gensim\n",
    "from keras.layers import Flatten, Dropout\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "\n",
    "#Please note that spacy module needs to be installed first to make sure that lda2vec works properly.\n",
    "from lda2vec import preprocess, Corpus\n",
    "logging.basicConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('./all-the-news/articles1.csv', index_col = False)\n",
    "df2=pd.read_csv('./all-the-news/articles2.csv', index_col = False)\n",
    "df3=pd.read_csv('./all-the-news/articles3.csv', index_col = False)\n",
    "df=df1\n",
    "df=df1.append(df2) \n",
    "df=df.append(df3)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# df=df.drop(['Unnamed: 0'], axis=1).reset_index(drop=True)\n",
    "df = df.dropna(subset=['content']).reset_index(drop=True)#drop rows where col value is nan\n",
    "df = df.loc[df['content']!=' '].reset_index(drop=True)\n",
    "df = df.loc[df['content']!=''].reset_index(drop=True)\n",
    "# print(df[df[\"content\"].isnull()])   #this statement will return null now because we have already dropped null/nan values\n",
    "# print(\"loaded\")\n",
    "df = df.rename(columns={'content': 'body'})\n",
    "texts = list(df['body'])\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# Remove tokens with these substrings\n",
    "bad = set([\"ax>\", '`@(\"', '---', '===', '^^^'])\n",
    "\n",
    "\n",
    "def clean(line):\n",
    "    return ' '.join(w for w in line.split() if not any(t in w for t in bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "max_length = 10000   # Limit of 10k words per document\n",
    "# Convert to unicode (spaCy only works with unicode)\n",
    "texts = [str(clean(d)) for d in texts if len(str(clean(d))) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens and vocabulary loaded\n"
     ]
    }
   ],
   "source": [
    "#tokens, vocab = preprocess.tokenize(texts, max_length, merge=False,\n",
    "#                                    n_threads=4)\n",
    "\n",
    "tokens = np.load(\"tokens.npy\")\n",
    "vocab = np.load(\"vocab.npy\")\n",
    "\n",
    "print(\"tokens and vocabulary loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "# Make a ranked list of rare vs frequent words\n",
    "corpus.update_word_count(tokens)\n",
    "corpus.finalize()\n",
    "# The tokenization uses spaCy indices, and so may have gaps\n",
    "# between indices for words that aren't present in our dataset.\n",
    "# This builds a new compact index\n",
    "compact = corpus.to_compact(tokens)\n",
    "# Remove extremely rare words\n",
    "pruned = corpus.filter_count(compact, min_count=30)\n",
    "# Convert the compactified arrays into bag of words arrays\n",
    "bow = corpus.compact_to_bow(pruned)\n",
    "# Words tend to have power law frequency, so selectively\n",
    "# downsample the most prevalent words\n",
    "clean = corpus.subsample_frequent(pruned)\n",
    "# Now flatten a 2D array of document per row and word position\n",
    "# per column to a 1D array of words. This will also remove skips\n",
    "# and OoV words\n",
    "doc_ids = np.arange(pruned.shape[0])\n",
    "flattened, (doc_ids,) = corpus.compact_to_flat(pruned, doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality of the resulting word vectors\n",
    "num_features = 300\n",
    "#Minimum word count threshold\n",
    "min_word_count = 1\n",
    "#Number of threads to run in parallel\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "#Context window length\n",
    "context_size = 2\n",
    "#Seed for the RNG, to make the result reproducible\n",
    "seed = 1\n",
    "#number of negative samples to be drawn\n",
    "neg_samples = 10\n",
    "\n",
    "sentences = [nltk.word_tokenize(title) for title in texts]\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers, \n",
    "    size=num_features, \n",
    "    min_count=min_word_count,\n",
    "    negative=neg_samples,\n",
    "    window=context_size,\n",
    "    iter=20,\n",
    "    )\n",
    "\n",
    "word2vec_model.save('./word_vecs.pkl')\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert flattened.min() >= 0\n",
    "# Fill in the pretrained word vectors\n",
    "n_dim = 300\n",
    "trained_wordvc = './word_vecs.pkl'\n",
    "vectors, s, f = corpus.compact_word_vectors(vocab, filename=trained_wordvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump all preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "# Save all of the preprocessed files\n",
    "pickle.dump(vocab, open('vocab.pkl', 'w'))\n",
    "pickle.dump(corpus, open('corpus.pkl', 'w'))\n",
    "np.save(\"flattened\", flattened)\n",
    "np.save(\"doc_ids\", doc_ids)\n",
    "np.save(\"pruned\", pruned)\n",
    "np.save(\"bow\", bow)\n",
    "np.save(\"vectors\", vectors)\n",
    "print(\"saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
